{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q2_1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcc7UMBn7NW2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "ab2e00f8-30ed-4e66-9049-6d68f76c1c26"
      },
      "source": [
        "#חלק 1\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "#גישה לדרייב\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "#קריאת התמונות\n",
        "labels = ['PNEUMONIA', 'NORMAL']\n",
        "img_size = 150\n",
        "def get_training_data(data_dir):\n",
        "    data = [] \n",
        "    for label in labels: \n",
        "        path = os.path.join(data_dir, label)\n",
        "        class_num = labels.index(label)\n",
        "        counter = 0\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # שינוי התמונה לגודל 150 על 150\n",
        "                data.append([resized_arr, class_num])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return np.array(data)\n",
        "\n",
        "#טעינת התמונות\n",
        "train = get_training_data('/content/drive/My Drive/Deep_Learning/DATA/chest_xray/chest_xray/train')\n",
        "test = get_training_data('/content/drive/My Drive/Deep_Learning/DATA/chest_xray/chest_xray/test')\n",
        "val = get_training_data('/content/drive/My Drive/Deep_Learning/DATA/chest_xray/chest_xray/val')\n",
        "\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_val = []\n",
        "y_val = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "#חלוקת התמונות לפי תוויות - 2 בסה'\"כ\"\n",
        "for feature, label in train:\n",
        "    x_train.append(feature)\n",
        "    y_train.append(label)\n",
        "\n",
        "for feature, label in test:\n",
        "    x_test.append(feature)\n",
        "    y_test.append(label)\n",
        "    \n",
        "for feature, label in val:\n",
        "    x_val.append(feature)\n",
        "    y_val.append(label)\n",
        "\n",
        "# Normalize the data\n",
        "x_train = np.array(x_train) / 255\n",
        "x_val = np.array(x_val) / 255\n",
        "x_test = np.array(x_test) / 255\n",
        "\n",
        "# resize data for deep learning \n",
        "x_train = x_train.reshape(-1, img_size, img_size, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_val = x_val.reshape(-1, img_size, img_size, 1)\n",
        "y_val = np.array(y_val)\n",
        "\n",
        "x_test = x_test.reshape(-1, img_size, img_size, 1)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# With data augmentation to prevent overfitting and handling the imbalance in dataset\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip = True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n",
            "OpenCV(4.1.2) /io/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkKDaQyEjV4K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1b4899e-f705-4ec1-8990-44ee0815dbd4"
      },
      "source": [
        "#חלק 2\n",
        "#יצירת השכבות\n",
        "Earlystopping = keras.callbacks.EarlyStopping(monitor=\"accuracy\", min_delta=0,patience=5, verbose=0,mode=\"auto\",baseline=None, restore_best_weights=False,)  \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (150,150,1))) # dropout 0.1 + early stopping\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(64 , (5,5) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(BatchNormalization()) # new addition #2 <----\n",
        "model.add(Conv2D(64 , (5,5) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.1)) # new addition #1 <-----\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 128 , activation = 'relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
        "model.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy']) # opt is an sgd with lr = 0.1 \n",
        "model.summary()\n",
        "\n",
        "#קביעת פרמטרי לימוד והרצה\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\n",
        "history = model.fit(datagen.flow(x_train,y_train, batch_size = 32) ,epochs = 100 , validation_data = datagen.flow(x_val, y_val) ,callbacks = [Earlystopping,learning_rate_reduction])\n",
        "\n",
        "print(\"Loss of the model is - \" , model.evaluate(x_test,y_test)[0])\n",
        "print(\"Accuracy of the model is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
        "\n",
        "#ניתוח של התוצאות\n",
        "#epochs = [i for i in range(16)]\n",
        "#fig , ax = plt.subplots(1,2)\n",
        "#train_acc = history.history['accuracy']\n",
        "#train_loss = history.history['loss']\n",
        "#val_acc = history.history['val_accuracy']\n",
        "#val_loss = history.history['val_loss']\n",
        "#fig.set_size_inches(20,10)\n",
        "\n",
        "#ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
        "#ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n",
        "#ax[0].set_title('Training & Validation Accuracy')\n",
        "#ax[0].legend()\n",
        "#ax[0].set_xlabel(\"Epochs\")\n",
        "#ax[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "#ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n",
        "#ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n",
        "#ax[1].set_title('Testing Accuracy & Loss')\n",
        "#ax[1].legend()\n",
        "#ax[1].set_xlabel(\"Epochs\")\n",
        "#ax[1].set_ylabel(\"Training & Validation Loss\")\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 150, 150, 32)      320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 150, 150, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 75, 75, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 75, 75, 64)        51264     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 75, 75, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 38, 38, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 38, 38, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 38, 38, 64)        102464    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 38, 38, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 38, 38, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 19, 19, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 19, 19, 128)       73856     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 19, 19, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 19, 19, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 10, 10, 256)       295168    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 10, 10, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 10, 10, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               819328    \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,344,961\n",
            "Trainable params: 1,343,745\n",
            "Non-trainable params: 1,216\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "164/164 [==============================] - 441s 3s/step - loss: 0.4624 - accuracy: 0.8369 - val_loss: 9.2799 - val_accuracy: 0.5000\n",
            "Epoch 2/100\n",
            "164/164 [==============================] - 440s 3s/step - loss: 0.2772 - accuracy: 0.8970 - val_loss: 25.8033 - val_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.9156\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "164/164 [==============================] - 438s 3s/step - loss: 0.2304 - accuracy: 0.9156 - val_loss: 45.7705 - val_accuracy: 0.5000\n",
            "Epoch 4/100\n",
            "164/164 [==============================] - 440s 3s/step - loss: 0.1590 - accuracy: 0.9437 - val_loss: 1.2956 - val_accuracy: 0.5000\n",
            "Epoch 5/100\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9462\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "164/164 [==============================] - 441s 3s/step - loss: 0.1508 - accuracy: 0.9462 - val_loss: 14.4036 - val_accuracy: 0.5000\n",
            "Epoch 6/100\n",
            "164/164 [==============================] - 439s 3s/step - loss: 0.1339 - accuracy: 0.9527 - val_loss: 10.4666 - val_accuracy: 0.5000\n",
            "Epoch 7/100\n",
            "164/164 [==============================] - 438s 3s/step - loss: 0.1211 - accuracy: 0.9606 - val_loss: 1.6155 - val_accuracy: 0.5625\n",
            "Epoch 8/100\n",
            "164/164 [==============================] - 439s 3s/step - loss: 0.1177 - accuracy: 0.9583 - val_loss: 0.4425 - val_accuracy: 0.8750\n",
            "Epoch 9/100\n",
            "164/164 [==============================] - 441s 3s/step - loss: 0.1146 - accuracy: 0.9588 - val_loss: 0.7225 - val_accuracy: 0.6875\n",
            "Epoch 10/100\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9623\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n",
            "164/164 [==============================] - 440s 3s/step - loss: 0.1130 - accuracy: 0.9623 - val_loss: 9.0239 - val_accuracy: 0.5000\n",
            "Epoch 11/100\n",
            "164/164 [==============================] - 438s 3s/step - loss: 0.1043 - accuracy: 0.9615 - val_loss: 1.5253 - val_accuracy: 0.5000\n",
            "Epoch 12/100\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9634\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n",
            "164/164 [==============================] - 439s 3s/step - loss: 0.1036 - accuracy: 0.9634 - val_loss: 0.8167 - val_accuracy: 0.5625\n",
            "Epoch 13/100\n",
            "164/164 [==============================] - 440s 3s/step - loss: 0.0972 - accuracy: 0.9659 - val_loss: 1.3505 - val_accuracy: 0.5000\n",
            "Epoch 14/100\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9625\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-06.\n",
            "164/164 [==============================] - 438s 3s/step - loss: 0.1023 - accuracy: 0.9625 - val_loss: 0.9836 - val_accuracy: 0.5625\n",
            "Epoch 15/100\n",
            "164/164 [==============================] - 439s 3s/step - loss: 0.0968 - accuracy: 0.9665 - val_loss: 1.0532 - val_accuracy: 0.6250\n",
            "Epoch 16/100\n",
            "164/164 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9636\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "164/164 [==============================] - 440s 3s/step - loss: 0.1014 - accuracy: 0.9636 - val_loss: 1.1657 - val_accuracy: 0.5000\n",
            "Epoch 17/100\n",
            "164/164 [==============================] - 439s 3s/step - loss: 0.0930 - accuracy: 0.9657 - val_loss: 1.4271 - val_accuracy: 0.5625\n",
            "Epoch 18/100\n",
            "164/164 [==============================] - 437s 3s/step - loss: 0.1008 - accuracy: 0.9632 - val_loss: 1.5859 - val_accuracy: 0.5625\n",
            "Epoch 19/100\n",
            "164/164 [==============================] - 439s 3s/step - loss: 0.0923 - accuracy: 0.9661 - val_loss: 1.3133 - val_accuracy: 0.5625\n",
            "Epoch 20/100\n",
            "164/164 [==============================] - 439s 3s/step - loss: 0.0979 - accuracy: 0.9654 - val_loss: 1.4927 - val_accuracy: 0.5000\n",
            "20/20 [==============================] - 12s 622ms/step - loss: 0.2886 - accuracy: 0.9183\n",
            "Loss of the model is -  0.2886277139186859\n",
            "20/20 [==============================] - 12s 620ms/step - loss: 0.2886 - accuracy: 0.9183\n",
            "Accuracy of the model is -  91.82692170143127 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}